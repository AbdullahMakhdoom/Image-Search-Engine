{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature_extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1i0EzCKSrBP-hnf2JvAPaacbzHUp1Wy1H",
      "authorship_tag": "ABX9TyO/8PIuWWG8/yLlq9w7knI9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b17b5a9f75ad436c8ac837ae611a9510": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5aded430a0a14558985b7c44f1adf97e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d39d927f539745b18e1c4bc482cf1299",
              "IPY_MODEL_0b07616c7e734d889556149c0b87f05b"
            ]
          }
        },
        "5aded430a0a14558985b7c44f1adf97e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d39d927f539745b18e1c4bc482cf1299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_22f68e35db574ea19ff6ea350b2b5472",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 8677,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 8677,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7a93614833cf49c5b2035c050341bd59"
          }
        },
        "0b07616c7e734d889556149c0b87f05b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eb4910dddff54c9eb2ea54f3986b8c1a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8677/8677 [34:39&lt;00:00,  4.17it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db0f640e40734b73bafa2cb2cac39f08"
          }
        },
        "22f68e35db574ea19ff6ea350b2b5472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7a93614833cf49c5b2035c050341bd59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eb4910dddff54c9eb2ea54f3986b8c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db0f640e40734b73bafa2cb2cac39f08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdullahMakhdoom/Reverse_Image_Search/blob/main/feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUuLu6UzDtyq"
      },
      "source": [
        "### Feature Vectors \n",
        "\n",
        "  The feature vectors (also called embeddings or bottleneck features) are essentially a collection of a few thousand floating-point values. Going through the convolution and pooling layers in a CNN is basically an act of reduction, to filter the information contained in the image to its most important and salient constituents, which in turn form the bottleneck features. Training the CNN molds these values in such a way that items (in our case: images) belonging to the same class have small Euclidean distance between them (or simply the square root of the sum of squares of the difference between corresponding values) and items from different classes are separated by larger distances. This is an important property which we will use in this project to develop an Image Retrival Enginer like Google's Image Search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDR2WbxIGiMt"
      },
      "source": [
        "**Objective** : In this notebook, we will extract 2 sets of feature vectors of Caltech101 Dataset using ResNet50 as deep architecture. One will be extracted using Resnet50 pre-trained on ImageNet. The other will be fine-tuned over Caltech101."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhZqlpI3tAWU"
      },
      "source": [
        "**Dataset** : Caltech 101 dataset has a size of 131 MB with approximately 9000 \n",
        "images in 101 categories (about 40 to 800 images per category). \n",
        "\n",
        "*Note* : *A 102nd category called \"BACKGROUND_Google\" consisting of random images is also present in this dataset,which needs to be deleted.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apLnAJlMsl-d"
      },
      "source": [
        "Download the Caltech101 dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKZ0vAYiwUDj",
        "outputId": "db45b35e-7378-46af-b17a-be86548aff99"
      },
      "source": [
        "!pip install gdown"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsRVi4JjscoO"
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp --output caltech101.tar.gz\n",
        "!tar -xvzf caltech101.tar.gz\n",
        "!mkdir datasets\n",
        "!mv 101_ObjectCategories datasets/caltech101\n",
        "!rm -rf datasets/caltech101/BACKGROUND_Google"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqaUWbz1f2Op"
      },
      "source": [
        "**Import Necessary Packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": false,
        "id": "w3-s1COTuFxV"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import pickle\n",
        "from tqdm import tqdm, notebook\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQaxAnijvbhs"
      },
      "source": [
        "A Utility function that allows us to choose a pretrained model architecture, pre-trained on ImageNet, with all the necessary details for experimentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hbo23jhtvaW0"
      },
      "source": [
        "def model_picker(name):\n",
        "    if (name == 'vgg16'):\n",
        "        model = VGG16(weights='imagenet',\n",
        "                      include_top=False,\n",
        "                      input_shape=(224, 224, 3),\n",
        "                      pooling='max')\n",
        "    elif (name == 'vgg19'):\n",
        "        model = VGG19(weights='imagenet',\n",
        "                      include_top=False,\n",
        "                      input_shape=(224, 224, 3),\n",
        "                      pooling='max')\n",
        "    elif (name == 'mobilenet'):\n",
        "        model = MobileNet(weights='imagenet',\n",
        "                          include_top=False,\n",
        "                          input_shape=(224, 224, 3),\n",
        "                          pooling='max',\n",
        "                          depth_multiplier=1,\n",
        "                          alpha=1)\n",
        "    elif (name == 'inception'):\n",
        "        model = InceptionV3(weights='imagenet',\n",
        "                            include_top=False,\n",
        "                            input_shape=(224, 224, 3),\n",
        "                            pooling='max')\n",
        "    elif (name == 'resnet'):\n",
        "        model = ResNet50(weights='imagenet',\n",
        "                         include_top=False,\n",
        "                         input_shape=(224, 224, 3),\n",
        "                        pooling='max')\n",
        "    elif (name == 'xception'):\n",
        "        model = Xception(weights='imagenet',\n",
        "                         include_top=False,\n",
        "                         input_shape=(224, 224, 3),\n",
        "                         pooling='max')\n",
        "    else:\n",
        "        print(\"Specified model not available\")\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iep7Pkz7vvP3"
      },
      "source": [
        "Picking 'ResNet50' as our model architecture. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ohIWXDlvvFi",
        "outputId": "a06beced-a68c-4470-c9cf-b50ce0484730"
      },
      "source": [
        "model_architecture = 'resnet'\n",
        "model = model_picker(model_architecture)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SqFUe7_wOas"
      },
      "source": [
        "Defining a function to extract image features from an image and a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjZ2V9G8wO6A"
      },
      "source": [
        "def extract_features(img_path, model):\n",
        "    input_shape = (224, 224, 3)\n",
        "    img = image.load_img(img_path,\n",
        "                         target_size=(input_shape[0], input_shape[1]))\n",
        "    img_array = image.img_to_array(img)\n",
        "    expanded_img_array = np.expand_dims(img_array, axis=0)\n",
        "    preprocessed_img = preprocess_input(expanded_img_array)\n",
        "    features = model.predict(preprocessed_img)\n",
        "    flattened_features = features.flatten()\n",
        "    normalized_features = flattened_features / norm(flattened_features)\n",
        "    return normalized_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVw3K9ghwa2r"
      },
      "source": [
        "Defining a helper function to recursively get all image files under a root directory.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8J9zpMPwYWG"
      },
      "source": [
        "extensions = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG']\n",
        "\n",
        "def get_file_list(root_dir):\n",
        "    file_list = []\n",
        "    for root, directories, filenames in os.walk(root_dir):\n",
        "        for filename in filenames:\n",
        "            if any(ext in filename for ext in extensions):\n",
        "                file_list.append(os.path.join(root, filename))\n",
        "    return file_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6iSo6mLw6FR"
      },
      "source": [
        "Running the extraction over the entire Caltech101 dataset and timing it using tqdm package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "b17b5a9f75ad436c8ac837ae611a9510",
            "5aded430a0a14558985b7c44f1adf97e",
            "d39d927f539745b18e1c4bc482cf1299",
            "0b07616c7e734d889556149c0b87f05b",
            "22f68e35db574ea19ff6ea350b2b5472",
            "7a93614833cf49c5b2035c050341bd59",
            "eb4910dddff54c9eb2ea54f3986b8c1a",
            "db0f640e40734b73bafa2cb2cac39f08"
          ]
        },
        "id": "qIRs-BHJw21o",
        "outputId": "e8ce3438-3da2-4e1a-b2a2-b5a85a0cc562"
      },
      "source": [
        "# path to the your datasets\n",
        "root_dir = 'datasets/caltech101'\n",
        "filenames = sorted(get_file_list(root_dir))\n",
        "\n",
        "feature_list = []\n",
        "for i in notebook.tqdm(range(len(filenames))):\n",
        "    feature_list.append(extract_features(filenames[i], model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b17b5a9f75ad436c8ac837ae611a9510",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=8677.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDLNVzh_OCxB"
      },
      "source": [
        "# make folder named 'data'\n",
        "!mkdir data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB1JT89HjsLq"
      },
      "source": [
        "Saving the features and filenames for later use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW_Aeno8jLLk"
      },
      "source": [
        "Utilizing parallel processing of GPUs will speed up the feature extraction step.\n",
        "(Keeping the batch_size = 64)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIfBXQFqOAXP",
        "outputId": "ea25672c-3c88-4a04-a96e-6e137be18090"
      },
      "source": [
        "batch_size = 64\n",
        "datagen = tensorflow.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "generator = datagen.flow_from_directory(root_dir,\n",
        "                                        target_size=(224, 224),\n",
        "                                        batch_size=batch_size,\n",
        "                                        class_mode=None,\n",
        "                                        shuffle=False)\n",
        "\n",
        "num_images = len(generator.filenames)\n",
        "num_epochs = int(math.ceil(num_images / batch_size))\n",
        "\n",
        "start_time = time.time()\n",
        "feature_list = []\n",
        "feature_list = model.predict_generator(generator, num_epochs)\n",
        "end_time = time.time()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8677 images belonging to 101 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-JdGr8rO9Tl"
      },
      "source": [
        "for i, features in enumerate(feature_list):\n",
        "    feature_list[i] = features / norm(features)\n",
        "\n",
        "feature_list = feature_list.reshape(num_images, -1)\n",
        "\n",
        "print(\"Num images   = \", len(generator.classes))\n",
        "print(\"Shape of feature_list = \", feature_list.shape)\n",
        "print(\"Time taken in sec = \", end_time - start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2UmLo_KgPUi"
      },
      "source": [
        "Utilizing Colab's GPU fastened the feature extraction process approximately 3 times (4080(sec)/ 1331(sec) = 3.065)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvY_wyGMxCFF"
      },
      "source": [
        "pickle.dump(feature_list, open('data/features-caltech101-resnet.pickle', 'wb'))\n",
        "pickle.dump(filenames, open('data/filenames-caltech101.pickle', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD_iMAwAhd8X"
      },
      "source": [
        "**Fine-tuned Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Bs7ey7R9uFxn"
      },
      "source": [
        "TRAIN_SAMPLES = 8677\n",
        "NUM_CLASSES = 101\n",
        "IMG_WIDTH, IMG_HEIGHT = 224, 224"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep5PfIFoUdKz"
      },
      "source": [
        "train_datagen = ImageDataGenerator(preprocessing_function = preprocess_input,\n",
        "                                     rotation_range=20,\n",
        "                                     width_shift_range=0.2,\n",
        "                                     height_shift_range=0.2,\n",
        "                                     zoom_range=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcgjGI6MiLX7"
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(root_dir,\n",
        "                                                    target_size=(IMG_WIDTH,\n",
        "                                                                 IMG_HEIGHT),\n",
        "                                                    batch_size = 64,\n",
        "                                                    shuffle = True,\n",
        "                                                    seed = 42,\n",
        "                                                    class_mode= 'categorical')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39McpnrMiLVM"
      },
      "source": [
        "def model_maker():\n",
        "  base_model = ResNet50(include_top = False,\n",
        "                        input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "  for layer in base_model.layers[:]:\n",
        "    layer.trainable = False\n",
        "  input = Input(shape = (IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "  custom_model = base_model(input)\n",
        "  custom_model = GlobalAveragePooling2D()(custom_model)\n",
        "  custom_model = Dense(64, activation='relu')(custom_model)\n",
        "  custom_model = Dropout(0.5)(custom_model)\n",
        "  predictions = Dense(NUM_CLASSES, activation='softmax')(custom_model)\n",
        "  return Model(inputs=input, outputs=predictions)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za1PJgtIiLTB",
        "outputId": "ecd8604d-2398-49e6-9b32-1ae8678cd102"
      },
      "source": [
        "model_finetuned = model_maker()\n",
        "model_finetuned.compile(loss = 'categorical_crossentropy',\n",
        "                        optimizer=tensorflow.keras.optimizers.Adam(0.001),\n",
        "                        metrics=['acc'])\n",
        "model_finetuned.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = math.ceil(float(TRAIN_SAMPLES) / batch_size),\n",
        "    epochs = 10)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "136/136 [==============================] - 1371s 10s/step - loss: 3.4368 - acc: 0.2723\n",
            "Epoch 2/10\n",
            "136/136 [==============================] - 1391s 10s/step - loss: 1.7939 - acc: 0.5703\n",
            "Epoch 3/10\n",
            "136/136 [==============================] - 1381s 10s/step - loss: 1.3258 - acc: 0.6644\n",
            "Epoch 4/10\n",
            "136/136 [==============================] - 1389s 10s/step - loss: 1.1050 - acc: 0.7030\n",
            "Epoch 5/10\n",
            "136/136 [==============================] - 1388s 10s/step - loss: 0.9940 - acc: 0.7311\n",
            "Epoch 6/10\n",
            "136/136 [==============================] - 1387s 10s/step - loss: 0.8527 - acc: 0.7579\n",
            "Epoch 7/10\n",
            "136/136 [==============================] - 1386s 10s/step - loss: 0.8143 - acc: 0.7631\n",
            "Epoch 8/10\n",
            "136/136 [==============================] - 1381s 10s/step - loss: 0.7317 - acc: 0.7878\n",
            "Epoch 9/10\n",
            "121/136 [=========================>....] - ETA: 2:34 - loss: 0.7600 - acc: 0.7750"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qBNhTueneIi"
      },
      "source": [
        "model_finetuned.save('./data/model-finetuned.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWuUYnd4neFc"
      },
      "source": [
        "start_time = time.time()\n",
        "feature_list_finetuned = []\n",
        "feature_list_finetuned = model_finetuned.predict_generator(generator, num_epochs)\n",
        "end_time = time.time()\n",
        "\n",
        "for i, features_finetuned in enumerate(feature_list_finetuned):\n",
        "    feature_list_finetuned[i] = features_finetuned / norm(features_finetuned)\n",
        "\n",
        "feature_list = feature_list_finetuned.reshape(num_images, -1)\n",
        "\n",
        "print(\"Num images   = \", len(generator.classes))\n",
        "print(\"Shape of feature_list = \", feature_list.shape)\n",
        "print(\"Time taken in sec = \", end_time - start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfec21p5neCN"
      },
      "source": [
        "pickle.dump(\n",
        "    feature_list,\n",
        "    open('./data/features-caltech101-resnet-finetuned.pickle', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CZCOM6znevy"
      },
      "source": [
        "Saving '.pickle' files to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fvkwt4inedh",
        "outputId": "42632e3f-624d-452c-90fd-ff3c7d58617c"
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import driv\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVwcBwQuBjfb"
      },
      "source": [
        "# Make new directory in MyDrive\n",
        "!mkdir /content/drive/MyDrive/Caltech101-features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz63orZDoO0k"
      },
      "source": [
        "!mv ./data/features-caltech101-resnet-finetuned.pickle /content/drive/MyDrive/Caltech101-features/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5-CjWSjBvb2"
      },
      "source": [
        "!mv ./data/features-caltech101-resnet.pickle /content/drive/MyDrive/Caltech101-features/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0FucSKABvQ5"
      },
      "source": [
        "!mv ./data/filenames-caltech101.pickle /content/drive/MyDrive/Caltech101-features/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}